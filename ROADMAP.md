# Voice Mirror â€” Roadmap

The big picture: Voice Mirror starts as a desktop overlay, becomes a server you can access from any device, and eventually lives in your pocket.

---

## Where We Are Now (v0.8.x)

Electron desktop app â€” transparent overlay with orb, embedded Claude Code terminal, Python voice backend (STT/TTS/VAD), MCP server with 58 tools. Works on Windows, macOS, Linux.

Key architectural advantage: the heavy lifting already runs as **separate processes** (Python backend, AI provider PTY, MCP server). The Electron shell is just the UI and orchestration layer.

---

## Phase 1: Voice Mirror TUI (Local Model Dashboard)

**Goal:** Replace the blank terminal canvas with a proper TUI dashboard when using local models (Ollama, LM Studio, Jan, etc.). Inspired by [Shodh](https://github.com/varun29ankuS/shodh-memory)'s terminal dashboard.

Claude Code has its own TUI. Local models currently dump plain text into ghostty-web, leaving a huge blank area. Voice Mirror deserves its own TUI for these providers.

### Layout

```
â•­â”€ Voice Mirror â”€â”€â”€â”€ Ollama (mistral-3) â”€â”€â”€â”€â”€â”€ â— Running â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ â”Œâ”€ Conversation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€ Tool Calls â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚                                  â”‚ â”‚ âœ“ browser_search  2s   â”‚ â”‚
â”‚ â”‚  â–¸ You                 8:34 PM   â”‚ â”‚   "weather london"     â”‚ â”‚
â”‚ â”‚  What's the weather in London?   â”‚ â”‚                        â”‚ â”‚
â”‚ â”‚                                  â”‚ â”‚ âœ“ read_screen     1s   â”‚ â”‚
â”‚ â”‚  â–¸ mistral-3           8:34 PM   â”‚ â”‚   captured 1920x1080   â”‚ â”‚
â”‚ â”‚  Let me check that for you.      â”‚ â”‚                        â”‚ â”‚
â”‚ â”‚                                  â”‚ â”‚ â ‹ memory_store         â”‚ â”‚
â”‚ â”‚  [Tool: browser_search]          â”‚ â”‚   saving context...    â”‚ â”‚
â”‚ â”‚                                  â”‚ â”‚                        â”‚ â”‚
â”‚ â”‚  It's currently 12Â°C in London   â”‚ â”œâ”€ Info â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚ â”‚  with partly cloudy skies.       â”‚ â”‚ Model   mistral-3      â”‚ â”‚
â”‚ â”‚  Wind is 15mph from the west.    â”‚ â”‚ Temp    0.7            â”‚ â”‚
â”‚ â”‚                                  â”‚ â”‚ Speed   42 tok/s       â”‚ â”‚
â”‚ â”‚  â–¸ You                 8:36 PM   â”‚ â”‚ Tools   12 loaded      â”‚ â”‚
â”‚ â”‚  Thanks!                         â”‚ â”‚                        â”‚ â”‚
â”‚ â”‚                                  â”‚ â”‚ â–¶ Speaking...          â”‚ â”‚
â”‚ â”‚  â–¸ mistral-3           8:36 PM   â”‚ â”‚                        â”‚ â”‚
â”‚ â”‚  â–ˆ (streaming...)                â”‚ â”‚                        â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CTX: 2.1K/32K â”‚ TTS: Edge (Aria) â”‚ STT: Parakeet â”‚ 3 tool callsâ”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
```

### Panels

| Panel | Content |
|-------|---------|
| **Header** | Model name, status badge (Running/Stopped), connection indicator |
| **Conversation** (left) | Scrollable chat â€” user/assistant messages with timestamps, streaming cursor, tool call inline markers |
| **Tool Calls** (right top) | Live activity feed â€” each MCP tool call with status icon (â ‹ running, âœ“ done, âœ— failed), name, duration, one-line detail. Scrollable. Most recent at top |
| **Info** (right bottom) | Model name, temperature, generation speed (tok/s), tool count, TTS/voice status |
| **Status bar** (bottom) | Context usage bar, TTS engine + voice, STT engine, tool call count |

### Implementation

**No separate process.** The openai-provider already writes to ghostty-web via `emitOutput('stdout', text)`. Instead of plain text, it outputs ANSI escape sequences to render the TUI.

#### 1A: TUI Renderer Module
New `electron/providers/tui-renderer.js`:

- Tracks terminal dimensions (cols Ã— rows), updated on resize
- Maintains state: message history, tool calls, model info, streaming buffer
- `render()` â€” full screen repaint using ANSI escape codes (cursor positioning, colors, box-drawing characters `â•­â•®â•°â•¯â”‚â”€â”Œâ”â””â”˜â”œâ”¤`)
- `appendMessage(role, text, timestamp)` â€” adds to chat, triggers partial re-render
- `streamToken(token)` â€” appends to current streaming message (no full repaint)
- `addToolCall(name, status, detail)` â€” updates tool calls panel
- `updateInfo(key, value)` â€” updates info panel fields
- Word-wrapping and scroll offset for both chat and tool panels

#### 1B: Wire into OpenAI Provider
Modify `electron/providers/openai-provider.js`:

- On start: create TUI renderer, do initial full render
- On user input: `tui.appendMessage('user', text)`
- On streaming tokens: `tui.streamToken(token)` (character-by-character)
- On complete response: `tui.appendMessage('assistant', fullText)`
- On tool call: `tui.addToolCall('browser_search', 'running', '"weather london"')`
- On tool result: `tui.updateToolCall('browser_search', 'done', '2s')`
- On resize: `tui.resize(cols, rows)` â†’ full re-render
- Generation speed calculated from token count / elapsed time

#### 1C: Terminal Input Handling
The TUI needs to handle keyboard input from ghostty-web:

- Scroll chat: `j`/`k` or arrow keys
- Switch panel focus: `Tab`
- Scroll tool calls when focused
- All other input passes through to the provider as normal

#### 1D: Theming
Use the existing Voice Mirror color palette (from CSS variables) mapped to ANSI 256-color codes:

- Accent color (`#667eea`) for borders, headers
- Muted colors for timestamps, secondary text
- Green/red/yellow for status indicators
- Background matches the terminal background

### Applies To
All non-PTY providers: Ollama, LM Studio, Jan, OpenAI API, Gemini API, Groq, Mistral, OpenRouter, DeepSeek, Grok â€” anything that goes through `openai-provider.js`.

Claude Code and OpenCode keep their own TUI (they're PTY-based).

---

## Phase 1.5: Custom Wake Word â€” "Mirror"

**Goal:** Replace the default "Hey Claude" wake word with a custom **"Mirror"** keyword, giving Voice Mirror its own identity independent of any AI provider.

### Why
The current wake word detection uses [OpenWakeWord](https://github.com/dscripka/openWakeWord) with pre-trained models ("hey_claude", "hey_jarvis", etc.). "Hey Claude" ties the experience to one provider â€” but Voice Mirror works with Ollama, GPT, Gemini, and others. A custom wake word makes it provider-agnostic and more natural: just say **"Mirror"** and start talking.

### Training a Custom Model

OpenWakeWord supports training custom models. The pipeline:

1. **Collect positive samples** â€” record ~100+ clips of "Mirror" spoken by different people, accents, distances, background noise levels
2. **Generate synthetic samples** â€” use TTS engines (Piper, Kokoro, Edge TTS) to generate thousands of synthetic "Mirror" utterances with varied voices, speeds, and pitches
3. **Collect negative samples** â€” ambient noise, speech that sounds similar ("bitter", "litter", "mere"), general conversation
4. **Train with OpenWakeWord** â€” fine-tune a small model (~500KB) using the provided training notebook
5. **Validate** â€” test false positive rate (activations on non-wake-word speech) and false negative rate (missed activations)
6. **Ship** â€” bundle the `.onnx` model file with Voice Mirror, add "Mirror" to the wake word selector in settings

### Technical Details

- **Model format:** ONNX (same as existing wake word models)
- **Model size:** ~500KB (runs on CPU, <1ms inference)
- **Integration point:** `python/wake_word.py` â€” already loads `.onnx` models from a configurable path
- **Settings:** Add "Mirror" option to wake word dropdown alongside existing options
- **Fallback:** Keep "Hey Claude", "Hey Jarvis", etc. as alternatives â€” user picks in settings
- **Threshold tuning:** Expose sensitivity slider in settings (higher = fewer false positives, lower = fewer misses)

### Stretch Goals
- **Custom wake word training UI** â€” let users record their own wake word samples in-app and train a personal model
- **Multiple wake words** â€” respond to both "Mirror" and a user-defined phrase
- **Confirmation sound** â€” play a subtle chime when wake word is detected (before STT starts)

---

## Phase 1.6: Embedded n8n Dashboard

**Goal:** Bring n8n's workflow automation directly into Voice Mirror â€” visible, live, and voice-controllable. Users see workflows execute in real-time without leaving the app.

### Why This Matters

n8n is the automation backbone that makes Voice Mirror genuinely useful beyond conversation. Through MCP tools, any AI provider (Claude Code, Ollama, GPT, etc.) can trigger n8n workflows via voice:

- *"Build me a workflow that sends a daily email summary of my GitHub PRs"* â†’ Claude Code creates the n8n workflow using MCP tools
- *"Send an email to the team about the deploy"* â†’ triggers a Gmail n8n workflow
- *"When someone stars my repo, post to Slack"* â†’ creates a webhook-driven automation
- *"Check my calendar and read out today's meetings"* â†’ Google Calendar workflow

The power is that **frontier LLMs like Claude can build n8n workflows from scratch** using the MCP n8n tools. Users don't need to know n8n's node editor â€” they just describe what they want by voice, and the AI builds the automation for them. This turns Voice Mirror into a voice-driven automation platform.

### Layout

The Browser sidebar tab gets a sub-navigation:

```
Sidebar                    Main panel
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Chat             â”‚       â”‚ â† â†’  ğŸ”„  localhost:5678       â”‚
â”‚ >_ Ollama â—     â”‚       â”‚                                â”‚
â”‚ ğŸŒ Browser      â”‚       â”‚  â”Œâ”€â”€â”€ n8n Workflow Editor â”€â”€â”€â” â”‚
â”‚    â”œâ”€ Web       â”‚       â”‚  â”‚                           â”‚ â”‚
â”‚    â””â”€ n8n â—     â”‚       â”‚  â”‚  [Webhook] â†’ [OpenAI]    â”‚ â”‚
â”‚ âš™ Settings      â”‚       â”‚  â”‚       â†“                   â”‚ â”‚
â”‚                 â”‚       â”‚  â”‚  [Gmail] â†’ [Slack Post]   â”‚ â”‚
â”‚                 â”‚       â”‚  â”‚                           â”‚ â”‚
â”‚                 â”‚       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                 â”‚       â”‚  âœ“ Workflow executed (2.3s)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- **Web** â€” current browser (Google searches, web pages)
- **n8n** â€” embedded n8n dashboard on `localhost:5678`
- Green dot when n8n is detected and running
- Auto-switches to n8n view when an n8n MCP tool fires

### User Setup Flow

1. **Install n8n** â€” Voice Mirror Settings shows an "n8n" section with a one-click install button (`npm install -g n8n`) or detects existing installation
2. **First run** â€” n8n requires a free account (email signup at n8n.io â†’ activation key emailed)
3. **Configure in Voice Mirror** â€” paste the n8n API key in Settings; URL defaults to `localhost:5678`
4. **Import templates** â€” Voice Mirror offers to pre-load starter workflow templates (Gmail, Slack, Calendar, GitHub, etc.)
5. **Done** â€” n8n dashboard visible in-app, MCP tools connected, ready for voice commands

### Implementation

#### 1.6A: Browser Sub-Navigation
- Add **Web** / **n8n** toggle under the Browser nav item in sidebar
- n8n tab loads `localhost:5678` in the existing webview
- Status indicator: green dot when n8n responds to health check, gray when offline

#### 1.6B: n8n Detection & Auto-Start
- On app launch, ping `localhost:5678/healthz` to detect running n8n
- If not running but `n8n` command is available, offer to start it as a subprocess (same pattern as Python backend)
- Manage n8n lifecycle: start on app launch, stop on app quit
- Settings: n8n URL (default `http://localhost:5678`), API key, auto-start toggle

#### 1.6C: Auto-Navigate on Tool Call
- When an n8n MCP tool fires (e.g., `n8n_execute_workflow`, `n8n_create_workflow`), auto-switch the browser panel to n8n view
- Highlight the active workflow in the embedded dashboard
- After execution completes, show result status in the browser panel header

#### 1.6D: n8n Settings & Setup Wizard
- Settings section: n8n URL, API key, auto-start on/off
- Setup wizard: detects n8n installation, guides through account creation, API key entry
- "Install n8n" button that runs `npm install -g n8n` (with user confirmation)
- "Import Templates" button that loads Voice Mirror starter workflows

#### 1.6E: Starter Workflow Templates
Pre-built n8n workflows optimized for Voice Mirror:

| Template | Trigger | What it does |
|----------|---------|-------------|
| **Gmail Summary** | Schedule/Voice | Fetches unread emails, summarizes via AI, reads aloud |
| **Send Email** | Voice command | Composes and sends email via Gmail API |
| **Calendar Today** | Voice command | Reads today's Google Calendar events |
| **GitHub Notifications** | Webhook/Voice | Checks GitHub notifications, summarizes PRs |
| **Slack Message** | Voice command | Posts a message to a Slack channel |
| **Web Scraper** | Voice command | Fetches and extracts data from a URL |
| **Custom Webhook** | External trigger | Receives webhooks and notifies via Voice Mirror |

These templates are JSON files shipped with Voice Mirror, importable via the n8n API.

### n8n as the Task & Reminder Engine

n8n isn't just for automations â€” it **is** Voice Mirror's built-in task scheduler, reminder system, and recurring job runner. No custom heartbeat or cron system needed.

#### How It Works

```
User: "Wake me up at 8am and tell me about traffic"
    â†“
AI builds n8n workflow via MCP tools:
    [Schedule Trigger: 8:00 AM daily]
        â†’ [Google Maps API: traffic for saved route]
        â†’ [HTTP Request: POST to Voice Mirror webhook]
            Body: { "speak": "Good morning! Traffic on your commute is light, 25 minutes today." }
    â†“
Voice Mirror webhook listener receives callback â†’ TTS speaks the message
```

#### Webhook Listener

A lightweight HTTP endpoint (~50 lines) inside Voice Mirror that receives n8n callbacks:

```js
// POST /api/webhook  â†’  { "speak": "...", "notify": true }
// Voice Mirror receives it â†’ TTS speaks the text, optionally shows a notification
```

- Runs on `localhost:3334` (configurable)
- Accepts `speak` (text to say aloud), `notify` (show UI notification), `data` (structured payload)
- n8n workflows use the "HTTP Request" node to POST to this endpoint
- Works with any n8n trigger: schedule, webhook, email received, RSS feed, etc.

#### Example Voice-Created Workflows

| Voice Command | n8n Workflow Created |
|---|---|
| *"Wake me up at 8am with the weather"* | Schedule 8:00 â†’ Weather API â†’ webhook `/api/webhook` with forecast text |
| *"Remind me to take meds at 9am and 9pm"* | Two schedule triggers â†’ webhook with reminder message |
| *"Tell me when someone stars my repo"* | GitHub webhook â†’ webhook to Voice Mirror with star count |
| *"Every Friday, summarize my unread emails"* | Schedule Fri 5pm â†’ Gmail API â†’ AI summarize â†’ webhook with summary |
| *"Ping me if my server goes down"* | HTTP poll every 5 min â†’ if status != 200 â†’ webhook with alert |

#### Mobile Integration

This architecture extends naturally to the mobile app (Phase 4):

```
Phone (anywhere)                    Desktop (home)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Voice Mirror  â”‚   WebSocket      â”‚ Voice Mirror Server   â”‚
â”‚ Mobile App    â”‚ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚  â”œâ”€â”€ n8n (scheduler) â”‚
â”‚               â”‚                  â”‚  â”œâ”€â”€ Webhook listener â”‚
â”‚ "Remind me to â”‚                  â”‚  â”œâ”€â”€ Python TTS/STT   â”‚
â”‚  call mom at  â”‚                  â”‚  â””â”€â”€ AI provider      â”‚
â”‚  3pm"         â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â†“
                                   n8n creates schedule
                                   trigger â†’ 3pm fires â†’
                                   webhook â†’ TTS speaks
                                   "Time to call mom!"
                                   (+ push notification
                                    to phone)
```

Users set reminders from their phone by voice, n8n schedules them, and Voice Mirror speaks the reminder â€” whether the user is at their desktop or gets a push notification on mobile.

### What Makes This Unique

Most n8n users interact through a browser tab. Voice Mirror makes n8n **voice-first**:

1. **Build workflows by voice** â€” "Create a workflow that monitors my email for invoices and saves them to Google Drive" â†’ Claude Code builds the entire n8n workflow using MCP tools
2. **Trigger workflows by voice** â€” "Send the weekly report" â†’ executes the workflow
3. **Reminders & scheduling by voice** â€” "Wake me up at 8am with traffic" â†’ n8n handles the scheduling, Voice Mirror speaks the result
4. **See results in-app** â€” the n8n dashboard is right there, showing execution history
5. **No context switching** â€” everything lives inside Voice Mirror
6. **Mobile-ready** â€” set reminders from your phone, hear them on your desktop (or get push notifications)

This is the bridge between conversational AI and real-world automation. n8n is the scheduler, Voice Mirror is the voice.

---

## Phase 2: Voice Mirror Server

**Goal:** Run Voice Mirror as a Node.js server, access the full dashboard from `localhost:3333` in any browser.

```
npm start          â†’ Electron (orb, overlay, hotkeys â€” current)
npm run server     â†’ Server mode (browser dashboard on localhost)
```

### 1A: Transport Abstraction Layer
The renderer currently talks to the backend via Electron IPC (`window.voiceMirror.*` â†’ preload.js â†’ ipcMain). Create a transport abstraction so the same API works over both IPC and WebSocket.

```
Renderer code (unchanged)
    â†“
transport.js  â†’  IPC bridge (Electron mode)
              â†’  WebSocket bridge (Server mode)
    â†“
Backend services (unchanged)
```

- `electron/transport/ipc-transport.js` â€” current preload-based IPC (wrap existing)
- `electron/transport/ws-transport.js` â€” WebSocket client that mirrors the same API
- `electron/transport/index.js` â€” auto-detects environment and returns the right transport
- Renderer calls `transport.invoke('get-config')` instead of `ipcRenderer.invoke('get-config')`

### 1B: Server Entry Point
New `server/index.js` that replaces `electron/main.js` as the orchestrator:

- Express (or Fastify) serves the renderer HTML/CSS/JS as static files
- WebSocket server handles all `invoke` / `on` / `send` messages
- Starts the same services: Python backend, AI provider, inbox watcher, config
- No Electron dependency â€” runs on plain Node.js

### 1C: Terminal WebSocket Proxy
The embedded terminal (xterm.js + node-pty) needs a WebSocket bridge in server mode:

- Server spawns PTY (node-pty) and pipes data over WebSocket
- Browser connects xterm.js via `xterm-addon-attach`
- Same terminal experience, different transport

### 1D: Graceful Degradation
Electron-specific features get web alternatives:

| Electron Feature | Browser Alternative |
|---|---|
| Transparent overlay / orb | Disabled â€” dashboard is the UI |
| uiohook global hotkeys | In-page keyboard shortcuts |
| System tray | Status bar in dashboard header |
| Native file dialogs | `<input type="file">` or text input |
| Screen capture | Browser Screen Capture API (with permission) or disabled |
| Always-on-top | Regular browser window |

### 1E: Settings & Mode Awareness
- Config flag `runtime: "electron" | "server"` detected at startup
- UI conditionally hides/shows features based on runtime
- Server mode shows a top bar with connection status, no orb controls

---

## Phase 3: Remote Access

**Goal:** Access Voice Mirror from any device on your network (or beyond).

### 3A: LAN Access
- Server binds to `0.0.0.0` instead of `127.0.0.1` (opt-in setting)
- Access from phone, tablet, another PC: `http://192.168.x.x:3333`
- Simple auth token or password to prevent unauthorized access

### 3B: Secure Remote Access
- HTTPS with self-signed cert (auto-generated) or user-provided cert
- Optional tunnel integration (Cloudflare Tunnel, ngrok, Tailscale) for internet access
- Auth: API key, basic auth, or OAuth

### 3C: Multi-Client Support
- Multiple browser tabs/devices connected simultaneously
- Real-time state sync via WebSocket broadcast
- Chat history visible on all connected clients
- Only one client can hold the microphone at a time (PTT ownership)

---

## Phase 4: Mobile App

**Goal:** Voice Mirror on your phone â€” talk to Claude from anywhere.

### Architecture
The mobile app is a **thin client** that connects to the Voice Mirror server running on your desktop/server. It doesn't run Python or node-pty locally â€” it streams everything.

```
Phone (client)                    Desktop/Server (backend)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     WebSocket     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Voice Mirror â”‚ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ â”‚ Voice Mirror Server  â”‚
â”‚ Mobile App   â”‚                  â”‚  â”œâ”€â”€ Python backend   â”‚
â”‚              â”‚                  â”‚  â”œâ”€â”€ AI provider/PTY  â”‚
â”‚ - Microphone â”‚                  â”‚  â”œâ”€â”€ MCP server       â”‚
â”‚ - Speaker    â”‚                  â”‚  â””â”€â”€ Config/Services  â”‚
â”‚ - Chat UI    â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ - Settings   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4A: Mobile-Optimized Web UI (PWA)
Before building a native app, ship a Progressive Web App:

- Responsive layout of the existing dashboard (chat + settings)
- PWA manifest for "Add to Home Screen"
- Push notifications for AI responses **and n8n reminders** (Phase 1.6 webhook triggers push)
- MediaRecorder API for voice input (record on phone, stream to server)
- Audio playback for TTS responses streamed from server
- Voice-driven reminders: "Remind me at 3pm to call mom" â†’ server creates n8n workflow â†’ push notification at 3pm

This tests the full mobile experience with zero native code.

### 4B: Native Mobile App
If PWA limitations become blockers (background audio, always-on mic, Siri/Google Assistant integration):

- **React Native** or **Expo** â€” share component logic with web
- Native audio pipeline for better mic/speaker control
- Background voice processing
- Widgets (iOS/Android) for quick voice activation
- Platform-specific wake word detection (on-device)

### 4C: Voice Streaming Protocol
For real-time voice on mobile:

- Client records audio â†’ streams raw PCM/opus over WebSocket â†’ server runs STT
- Server generates TTS â†’ streams audio chunks back â†’ client plays
- Low-latency bidirectional audio streaming (target: <500ms round-trip)
- Fallback: record-then-send for high-latency connections

---

## Phase 5: Cloud Deployment (Optional / Future)

**Goal:** Voice Mirror as a hosted service â€” no local install needed.

- Docker image: `docker run -p 3333:3333 voice-mirror`
- Cloud deploy (Railway, Fly.io, AWS) with GPU for local STT/TTS models
- User accounts, persistent config, conversation history
- API key management (bring your own Claude/OpenAI keys)

This is the furthest out and depends on demand. The server architecture from Phase 2-3 makes this possible without major changes.

---

## Execution Priority

```
Phase 1 (TUI)            â€” Immediate visual impact. Fixes the blank terminal problem. âœ… Shipped v0.8.6
Phase 1.5 (Wake word)    â€” Brand identity. Can develop in parallel with anything.
Phase 1.6 (n8n embed)    â€” Voice-driven automation. High user value, builds on existing browser.
Phase 2 (Server mode)    â€” Foundation for cross-platform. Everything after builds on this.
Phase 3A (LAN access)    â€” Quick win once Phase 2 is done.
Phase 4A (PWA)           â€” Mobile access with minimal new code.
Phase 3B-C (Remote/Auth) â€” When users want internet access.
Phase 4B (Native app)    â€” Only if PWA hits real limitations.
Phase 5 (Cloud)          â€” If there's demand for hosted Voice Mirror.
```

Phase 1 (TUI) shipped in v0.8.6. Phase 1.5 (wake word) and Phase 1.6 (n8n) are both independent â€” they can be developed in parallel with each other and with Phase 2. Phase 1.6 is particularly high-value because it turns Voice Mirror from a conversation tool into a **voice-driven automation and scheduling platform**: users say what they want built, and the AI creates n8n workflows that connect to real services (Gmail, Slack, GitHub, etc.). The webhook listener means n8n can call back into Voice Mirror to speak reminders, alerts, and summaries â€” making it a true personal assistant with memory and initiative. Combined with Phase 4 (mobile), users can set reminders from their phone and hear them spoken aloud at home.

---

## Non-Goals

- **Replace Electron entirely** â€” the desktop overlay experience (orb, hotkeys, always-on-top) is a core feature. Server mode is an alternative, not a replacement.
- **Run Python on mobile** â€” the phone is a thin client. STT/TTS processing stays on the server.
- **Build a general-purpose web framework** â€” the server is purpose-built for Voice Mirror, not a generic platform.
